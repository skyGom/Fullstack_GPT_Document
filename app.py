from langchain.schema.runnable import RunnablePassthrough, RunnableLambda
import Functions.document_llm_streamlit as dls
import streamlit as st

st.set_page_config(
    page_title="DocumentGPT",
    page_icon="ğŸ“–",
)

st.title("DocumentGPT")

with st.sidebar:
    api_key = st.text_input("OpenAI API keyë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.", type="password")
    if api_key:
        with st.spinner("API í‚¤ í™•ì¸ ì¤‘..."):
            valid_key = dls.is_valid_openai_key(api_key)
            
        if valid_key:
            _llm = dls.get_llm(api_key)
            file = st.file_uploader(
                ".txt .pdf í˜¹ì€ .docx ìœ í˜•ì˜ íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”.", type=["txt", "pdf", "docx"]
            )
        else:
            st.warning("API í‚¤ê°€ ìœ íš¨í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.")
    
    st.write("""
             ### Function.document_llm_streamlit.py ###
             class ChatCallBackHandler(BaseCallbackHandler):
                
                message = ""
                
                def on_llm_start(self, *args, **kwargs):
                    self.message_box = st.empty()
                        
                def on_llm_end(self, *arg, **kwargs):
                    save_message(self.message, "ai")
                        
                def on_llm_new_token(self, token, *arg, **kwargs):
                    self.message += token
                    self.message_box.markdown(self.message)
                    
            key = None

            def is_valid_openai_key(key):
                try:
                    openai.api_key = key
                    openai.Model.list()
                    return True
                except Exception as e:
                    return False
                
            def get_llm(api_key):
                key = api_key
                return ChatOpenAI(
                        model_name="gpt-4o-mini",
                        temperature=0.1,
                        streaming=True,
                        callbacks=[ChatCallBackHandler()],
                        openai_api_key=key,
                    )
                
            @st.cache_data(show_spinner="íŒŒì¼ì„ ì„ë² ë”© í•˜ê³ ìˆìŠµë‹ˆë‹¤...")
            def embed_file(file):
                file_content = file.read()
                file_path = f"./.cache/files/{file.name}"
                with open(file_path, "wb") as f:
                    f.write(file_content)
                cache_dir = LocalFileStore(f"./.cache/embeddings/{file.name}")
                splitter = CharacterTextSplitter.from_tiktoken_encoder(
                    separator="\\n",
                    chunk_size=600,
                    chunk_overlap=100,
                )
                loader = UnstructuredFileLoader(file_path)
                docs = loader.load_and_split(text_splitter=splitter)
                embeddings = OpenAIEmbeddings(openai_api_key=key)
                cached_embeddings = CacheBackedEmbeddings.from_bytes_store(embeddings, cache_dir)
                vectorstore = FAISS.from_documents(docs, cached_embeddings)
                return vectorstore.as_retriever()

            def save_message(message, role):
                st.session_state["messages"].append({"message": message, "role": role})

            def send_message(message, role, save=True):
                with st.chat_message(role):
                    st.markdown(message)
                if save:
                    save_message(message, role)

            def paint_history():
                for message in st.session_state["messages"]:
                    send_message(message["message"], message["role"], save=False)

            def format_docs(docs):
                return "\\n\\n".join(document.page_content for document in docs)

            prompt = ChatPromptTemplate.from_messages([
                    (
                        "system",
                        '''
                        ë‹¹ì‹ ì€ ë¬¸ì„œë¥¼ ì •í™•í•˜ê²Œ ë¶„ì„í•˜ëŠ” ì „ë¬¸ê°€ ì…ë‹ˆë‹¤. ë‹µë³€ì€ ë¬¸ì„œì— ê¸°ë°˜í•˜ì—¬ í•´ì£¼ì„¸ìš”. ëª¨ë¥´ëŠ” ë‚´ìš©ì€ ëª¨ë¥¸ë‹¤ê³  í•´ì£¼ì„¸ìš”. í•œêµ­ì–´ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”.
                        
                        Context: context
                        ''',
                    ),
                    ("human", "question"),
                ])
    ### app.py ###
        st.set_page_config(
        page_title="DocumentGPT",
        page_icon="ğŸ“–",
        )

        st.title("DocumentGPT")

        with st.sidebar:
            api_key = st.text_input("OpenAI API keyë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.", type="password")
            if api_key:
                with st.spinner("API í‚¤ í™•ì¸ ì¤‘..."):
                    valid_key = dls.is_valid_openai_key(api_key)
                    
                if valid_key:
                    _llm = dls.get_llm(api_key)
                    file = st.file_uploader(
                        ".txt .pdf í˜¹ì€ .docx ìœ í˜•ì˜ íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”.", type=["txt", "pdf", "docx"]
                    )
                else:
                    st.warning("API í‚¤ê°€ ìœ íš¨í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.")
            
            if file:
            retriever = dls.embed_file(file)
            dls.send_message("ì¤€ë¹„ì™„ë£Œ! ë¬´ì—‡ì´ë“  ë¬¼ì–´ë³´ì„¸ìš”", "ai", save=False)
            dls.paint_history()
            message = st.chat_input("ë‹¹ì‹ ì˜ ë¬¸ì„œì— ëŒ€í•´ ë¬´ì—‡ì´ë“  ë¬¼ì–´ë³´ì„¸ìš”...")
            if message:
                dls.send_message(message, "human")
                chain = (
                    {
                        "context": retriever | RunnableLambda(dls.format_docs),
                        "question": RunnablePassthrough(),
                    }
                    | dls.prompt
                    | _llm
                )
                with st.chat_message("ai"):
                    response = chain.invoke(message)
        else:
            st.markdown(
                '''
                        í™˜ì˜í•©ë‹ˆë‹¤!
                        
                        ì´ ì±—ë´‡ì„ ì‚¬ìš©í•˜ì—¬ AIì—ê²Œ ë¬¸ì„œ ê´€ë ¨ ì§ˆë¬¸ì„ í•´ë³´ì„¸ìš”!
                        
                        ì‚¬ì´ë“œ ë°”ì— ë¬¸ì„œë¥¼ ì—…ë¡œë“œí•˜ë©´ ì‹œì‘ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤!
                        '''
            )
            st.session_state["messages"] = []
            
    ### git repository url ###
        https://github.com/skyGom/Fullstack_GPT_Document
                """)

if file:
    retriever = dls.embed_file(file)
    dls.send_message("ì¤€ë¹„ì™„ë£Œ! ë¬´ì—‡ì´ë“  ë¬¼ì–´ë³´ì„¸ìš”", "ai", save=False)
    dls.paint_history()
    message = st.chat_input("ë‹¹ì‹ ì˜ ë¬¸ì„œì— ëŒ€í•´ ë¬´ì—‡ì´ë“  ë¬¼ì–´ë³´ì„¸ìš”...")
    if message:
        dls.send_message(message, "human")
        chain = (
            {
                "context": retriever | RunnableLambda(dls.format_docs),
                "question": RunnablePassthrough(),
            }
            | dls.prompt
            | _llm
        )
        with st.chat_message("ai"):
            response = chain.invoke(message)
else:
    st.markdown(
        """
                í™˜ì˜í•©ë‹ˆë‹¤!
                
                ì´ ì±—ë´‡ì„ ì‚¬ìš©í•˜ì—¬ AIì—ê²Œ ë¬¸ì„œ ê´€ë ¨ ì§ˆë¬¸ì„ í•´ë³´ì„¸ìš”!
                
                ì‚¬ì´ë“œ ë°”ì— ë¬¸ì„œë¥¼ ì—…ë¡œë“œí•˜ë©´ ì‹œì‘ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤!
                """
    )
    st.session_state["messages"] = []
